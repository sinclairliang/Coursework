## 13th November 2018 ##

### Big $O$ notation ###

A notation that describe how fast a function grows. It is also relative to the input size. 

Look at the following example:

```
linear search
find the element in an array from index 0 until we find the element.
```
It will take as long as the input's size. So we denote that to $O(n)$, or "linear time"

```
binary search
```
It will take as long as the input's size. So we denote that to $O(lg(n))$. 

### Drop the constants ###
When you're calculating the big O complexity of something, we just throw out the constants.

$O(2n)$, which we just call $O(n)$

$f(n)$ = ($10^9$ + $5000$ + $2n$) 

we call $O(f(n))$ = $O(n)$


### Hash table ###

An elegant way to manage data by implementing a (key, value) pair.

Depending on the `hash()` function we implement, they will be be collisions. How do we deal with collisions? One of the ways is by chaining. 

`chaining` we basically have an array of linkedlists so that we can link data that has the same key. Or another reason is to reduce the storage size, think we have 1000 elements to store, we can not realistically have 1000 slots to store each of them. We then use `%` `mod` operation. By doing this we can guarantee  we fit that many elements into certain amount of slots.

[![hash-table-chaining.png](https://i.postimg.cc/WzM59xJD/hash-table-chaining.png)](https://postimg.cc/Mfp0HP5x)


[![i-say-use-a-hashtable-and-90-of-the-time-im-right.jpg](https://i.postimg.cc/httgGscx/i-say-use-a-hashtable-and-90-of-the-time-im-right.jpg)](https://postimg.cc/svqb6pBD)